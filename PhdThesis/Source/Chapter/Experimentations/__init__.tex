\chapter{Expérimentations}
\label{chapter:experimentations}

\minitoc

Ce chapitre présente majoritairement deux expérimentations. Même si nous pouvons
noter la présence de certains chiffres, l'objectif n'est pas de faire des
expérimentations {\strong quantitatives} mais {\strong qualitatives}.
%Nous ne voulons pas montrer l'efficacité de notre approche par des chiffres sur
%des «~faux~» problèmes\footnote{Définir}.
Il est question dans ce mémoire d'introduire un langage de spécification pour
PHP, qui soit nouveau, simple, pragmatique et rassemblant plusieurs méthodes du
test. Le but est de présenter toutes ces méthodes et de simplifier leurs
utilisations dans le quotidien des ingénieurs de test et développeurs. C'est
pour cela que nous avons par ailleurs choisi une approche avec les contrats car
ils offrent beaucoup de services et demandent peu d'efforts de la part de
l'utilisateur.

Dans les chapitres précédents, nous avons présenté Praspel, un nouveau langage
de spécification pour PHP. Nous avons par la suite présenté différents
algorithmes de générations de données, inspirés de différentes méthodes du test.
Puis, nous avons défini des critères de couvertures sur ces contrats pour
mesurer la pertinence d'une suite de tests. Enfin, nous avons proposé une
implémentation de toutes ces contributions ainsi qu'une extension dans un outil
industriel répandu pour qu'elles soient utilisables dans le quotidien des
ingénieurs de test. C'est vers eux que nous allons nous tourner pour qu'ils
jugent notre travail. Nous pensons qu'obtenir une expertise humaine à travers
leurs retours et remarques mis en perspective par leur expérience est pertinent.

La première expérimentation a pour contexte un projet d'enseignement utilisant
un robot appelé UniTestor développé par nos soins dans le cadre des JDév' à
l'École Polytechnique. La seconde expérimentation a été réalisée auprès d'un
panel bénévol d'ingénieurs de test et d'entreprises afin d'obtenir des retours
concrets sur des programmes du «~monde réel~».

Ce chapitre se décompose de la manière suivante. Tout d'abord, dans la
partie~\ref{section:experimentation:unitestor}, nous présentons le cas d'étude
du robot UniTestor où nous comparons deux suites de tests~: une manuelle et une
générée automatiquement avec Praspel et son extension pour atoum. Nous verrons
que Praspel et ses outils rivalisent avec les tests manuels à couverture de code
équivalente mais avec moins de tests. Nous verrons également que le temps de
rédaction des contrats est nettement inférieur à l'écriture de tests manuels.
Puis, pour valider nos résultats sur des cas concrets, nous réappliquons une
méthodologie similaire dans la partie~\ref{section:experimentation:real} avec un
panel bénévol d'ingénieurs de test et d'entreprises. En plus des retours et
remarques, nous verrons que Praspel et ses outils ont permis de détecter des
erreurs dans du code déjà testé et en production, et ce rapidement et
efficacement. Enfin, …

\section{Étude de cas~: UniTestor}
\label{section:experimentation:unitestor}

Les Journées nationales du Développement Logiciel, abrégé JDév'\footnote{Voir
\url{http://devlog.cnrs.fr/jdev2013}.}, est une Action Nationale de Formation
inter-établissements, soutenue par la Mission pour l'interdisciplinité du
CNRS\footnote{Voir \url{http://mrct.cnrs.fr/}.}, l'INRIA\footnote{Voir
\url{http://www.inria.fr/}.} et l'INRA\footnote{Voir
\url{http://www.inra.fr/}.}, organisée en 2013 à l'École Polytechnique. Une
équipe de notre laboratoire a été invitée afin d'y donner des cours et des
ateliers. C'est dans ce cadre que nous avons développé UniTestor, un robot écrit
en PHP nous permettant d'aborder la majorité des aspects du tests unitaires~:
écrire un préambule de test, définir des assertions, boucher des objets etc. 16
élèves ont alors écrits manuellement une suite de tests pour ce robot, que nous
avons complété. Nous la comparons avec une suite de tests générée
automatiquement avec Praspel.

\subsection{Présentation générale}

UniTestor est une simulation de robot pour l'exploration spatiale. Son diagramme
de classes UML simplifié est présenté dans la
figure~\ref{figure:experimentation:unitestor}. Le robot est représenté par la
classe \code{Robot}. Il est constitué de plusieurs périphériques, que nous
allons détaillé, lui envoyant des informations. L'ensemble du robot comporte au
total 5~classes dont 21~méthodes, ce qui représente 340~lignes de code (sans les
contrats Praspel).
%
\begin{figure}

\fig{\textwidth}{!}{UniTestor.pdf}

\caption{\label{figure:experimentation:unitestor} Diagramme de classes UML du
robot UniTestor.}

\end{figure}

\subsubsection{Équipements et capteurs}

Le robot est capable de se déplacer en utilisant des coordonnées relatives ou
absolues avec respectivement les méthodes \code{move} et \code{moveTo}. Pour se
déplacer, il utilise une balise de géolocalisation représentée par la classe
\code{Coordinates}. À tout moment, il peut connaître la nature du terrain avec
le capteur de sol représenté par la classe \code{LandSensor}. La nature et la
configuration du terrain influent sur l'énergie utile à un déplacement. Le robot
a également sa propre horloge, représentée par la classe \code{Clock}, lui
permettant de connaître des informations sur le temps. Enfin, la classe
\code{Vector} permet de manipuler des paires de coordonnées.

\subsubsection{Les opérations de UniTestor}

La seule opération importante est le déplacement. Lorsqu'il se déplace, il
consomme de l'énergie, représentée par un pourcentage. En fonction du terrain,
plus ou moins d'énergie sera nécessaire. L'énergie nécessaire est calculée par
la fonction \code{getNeededEnergy} du capteur de sol. Mais le robot a des
batteries qui se recharge en fonction du temps. Le temps qui s'écoule est donné
par l'horloge et la recharge est linéaire (paramétrable par une constante).

\subsection{Protocole expérimental et résultats}

Nous avons commencé par annoter le code du robot par des contrats Praspel.
Ensuite, nous avons générer automatiquement une suite de tests, appelée la suite
AGT (\inenglish{Automatically Generated Tests}) satisfaisant le critère de
couverture de contrat \inenglish{All}-$G$ (défini dans la
partie~\ref{subsection:test:combination}). Nous avons comparé cette suite de
test avec la suite de tests manuels des étudiants, appelé la suite MT
(\inenglish{Manual Tests}), complétée pour atteindre un taux de couverture de
code de 100\% quand ce n'était pas le cas.

Nous avons observé que le nombre moyen de tests générés par les étudiants est de
53~MT, alors que pour 21 méthodes, soit 21 contrats, Praspel a généré 29~AGT,
soit une réduction de 45\%. Pour caractériser la pertinence des tests, nous
avons utiliser comme métrique la couverture de code. Le critère de couverture de
code le plus fin que nous propose atoum est le critère tous-les-arcs. Ainsi,
nous avons observé que les deux suites de tests avaient un score de 100\%. De
plus, 1h30 a été nécessaire aux étudiants pour rédiger en moyenne 53~tests,
alors qu'il nous a fallu 15~minutes pour écrire 21~contrats.  La phase de
génération de tests a pris moins d'une seconde~: cela comprend la génération
automatique des données de tests, à savoir des entiers, des réels, des chaînes
de caractères spécifiées par des expressions régulières et des objets, en
particulier plusieurs instances complètes du robot (avec ses périphériques).

Ainsi, nous obtenons le même score de couverture du code avec pratiquement
moitié moins de tests et en 6~fois moins de temps. Ces premiers résultats
montrent l'efficacité de l'approche du test automatisé et le gain qu'apporte les
contrats~: écrire des contrats est moins difficile et nécessite moins de temps
que d'écrire une suite de tests complète.

Cependant, cette expérimentation est biaisée~: les étudiants étaient en
apprentissage de l'outil et un temps d'adaptation leur était nécessaire. De
plus, nous sommes des experts de Praspel, donc nous rédigeons des contrats très
rapidement. Afin d'évaluer notre approche de manière plus objective, nous avons
demandé à un panel de bénévoles, composé d'ingénieurs de test et d'entreprises,
d'appliquer une expérimentation similaire sur leur propre code.

\section{Étude de cas concrète~: ingénieurs bénévoles}
\label{section:experimentation:real}

Introduction, bla bla.

\subsection{Présentation générale}

Nous avons lancé un appel sur différents réseaux sociaux et différentes
communautés pour rassembler des bénévoles pour cette expérimentation.
7~ingénieurs de test de nationalités différentes ont répondu à l'appel. Ces
ingénieurs font du test depuis en moyenne 5~ans. Ils ont appliqué
l'expérimentation sur leurs propres programmes ou sur des programmes sur
lesquels ils participent. Ces programmes constituent nos cas concrets. Voici la
description de ces programmes~:
%
\begin{itemize}

\item outil de \inenglish{debugging}~: analyse les performances d'exécutions de
certaines parties de programmes et génère des rapports dans deux formats
différents. Les parties abordées durant cette expérimentation étaient la mesure
des données et la génération des rapports.

\item outil de test pour le langage JSON~: une autre extension à atoum était en
développement pour proposer des nouvelles assertions sur le langage JSON. Les
parties abordées durant cette expérimentation étaient les assertions et les
tests de cette extension, c'est~à~dire s'assurer que l'outil de test testait
correctement le langage JSON.

\item outil d'échanges de messages~: les messages sont écrits en XML et
contiennent des données de toutes sortes, notamment des dates. C'est cette
partie qui a été majoritairement abordée durant cette expérimentation car
centrale à cet outil.

\item générateur de données de tests~: pour tester certaines parties techniques
d'un outil, un générateur de données de tests spécifique a été créé. Afin de
s'assurer que ce générateur ne comporte pas d'erreurs et étant lui-même assez
volumineux, il a sa propre suite de tests. Les parties qui ont été abordées
durant cette expérimentation étaient le calcul de ces données, les tests
unitaires et les tests fonctionnels.

\item outils de comptabilité~: 2~outils développés sur-mesure pour le calcul de
comptabilités. Les parties abordées étaient multiples.

\item outil d'évaluation de formules Mathématiques~: des utilisateurs écrivent
eux-mêmes des formules arithmétiques décrivant l'évolution des tarifs en temps
réels pour certains produits. Une bibliothèque de fonctions, de constantes et de
variables est mise à la disposition de ces utilisateurs. La partie abordée était
l'évaluation de ces formules analysées avec un compilateur et une grammaire
spécifique.

\end{itemize}

Tous ces programmes font intervenir tous les types de données, à savoir des
booléens, des entiers, des réels, des chaînes de caractères, des tableaux et des
objets. Dans le cas des chaînes de caractères, la plupart pouvaient être
spécifiées par une expression régulière, et quelqu'unes par des grammaires.

Sur ces programmes variés, nous avons demandé à notre panel d'appliquer les
protocoles expérimentaux suivants.

\subsection{Protocoles expérimentaux}

Cette expérimentation s'est déroulée en deux temps. Tout d'abord, nous avons
demandé au panel d'appliquer le protocole expérimental suivant~:
%
\begin{enumerate}

\item sélectionner plusieurs méthodes de toutes sortes déjà testées
manuellement (avec atoum)~;

\item annoter ces méthodes avec des contrats Praspel~;

\item générer automatiquement une suite de tests satisfaisant le critère de
couverture de contrat \inenglish{All}-$G$, et l'exécuter~;

\item comparer la suite MT (\inenglish{Manual Tests}) avec la suite AGT
(\inenglish{Automatically Generated Tests}).

\end{enumerate}

Puis, nous leur avons demandé d'expérimenter les algorithmes de générations de
données seuls, sans Praspel. Nous rappelons que l'extension introduisant Praspel
dans atoum permet la définition et l'utilisation des domaines réalistes
facilement (voir la partie~\ref{subsection:tools:extension}). Il n'y avait pas
de protocole expérimental pré-défini mais nous leur avons demandé d'observer
l'impact de certains générateurs sur la couverture de code ainsi que l'impact
sur les suites de tests (par exemple une réduction du nombres de tests).

L'expérimentation a été ouverte pendant une première semaine avec un premier
panel puis pendant deux autres semaines avec un second panel plus important. Les
parties suivantes regroupent toutes les données et informations des deux panels.

%On présente les ingénieurs et ce qu'on va faire~: groupe de méthodes, 2 TS etc.
%Que va-t-on comparer~? Les métriques sont~: nombres de tests, code coverage,
%temps.

\subsection{Comparaison de la couverture de code des suites de tests}

%Ici, on compare les suites de test en fonction de la couverture de tests.
%Regarder les données manipulées des deux côtés, comment ça a été testé (aux
%limites, aléatoirement etc.). Donner un sens aux chiffres (100\% ne veut rien
%dire). La couverture de code tous-les-arcs n'est probablement pas la meilleure,
%ça devrait se voir à un moment ou un autre.

Un moyen de comparer les suites de tests est d'utiliser la couverture de code
avec le critère tous-les-arcs, comme pour l'expérimentation précédente.  Les
résultats sont les suivants. Nous avons observé que les AGT couvrent autant que
les MT, à $\pm 5\%$ près.

Quand les AGT couvraient moins que les MT, le panel nous a fait remarquer que
c'était à cause de l'utilisation des bouchons (des \inenglish{mocks}). Un
bouchon remplace une partie d'un programme et nous prédéfinissons la manière
dont il va se comporter, c'est~à~dire que nous allons décrire un comportement
bien spécifique et tester le SUT dans ces conditions.  Les bouchons permettent
d'améliorer la testabilité des programmes (en plus d'accélérer l'exécution des
suites de tests par effet de bord). Les bouchons ne concernent que les objets ou
les fonctions, et Praspel n'est pas capable de générer de tels bouchons. Par
exemple, si un contrat précise qu'une exception peut être levée quand une
connexion réseau est interrompue, Praspel ne sera pas capable de couper lui-même
la connexion. Il faut alors boucher les interfaces réseaux et simuler des
déconnexions.

Et quand les AGT couvraient plus que les MT, c'était à cause d'une limitation
des outils. En effet, atoum ne permet pas de tester les méthodes avec une
visibilité autre que publique, alors que Praspel le permet. Le panel devait
alors tester les méthodes protégées et privées à travers les méthodes publiques.
Il est évident que certains chemins dans le code n'étaient pas accessibles
facilement.

Dans certains cas, malgré une couverture de code de 100\% pour les MT et les
AGT, des erreurs ont été trouvées. En effet, la couverture tous-les-arcs n'est
pas très fine et des erreurs peuvent apparaître derrière des conditions qui
n'auraient pas dû être activées. Comme le critère de couverture
\inenglish{All}-$G$ sur les contrats combinent tous les domaines réalistes de
toutes les variables, certaines combinaisons peuvent activer ces conditions
d'une manière qui n'avait pas été testée manuellement.

Maintenant, comparons le temps de rédactions de ces suites de tests.

\subsection{Temps de rédaction des suites de tests}

%Ils sont tous habitués à écrire des tests. Personne ne connaît Praspel. Est-ce
%qu'on compte le temps d'apprentissage~? Ils n'ont peut-être pas le temps
%d'écriture de la suite MT en tête~: on travaillera pas estimation.
%
%Faire 2 fois l'XP en fait. Une fois avec apprentissage, une fois sans.

Tous les membres du panel sont habitués à écrire des tests. Ils en écrivent des
dizaines par jour. Aucun d'entre eux n'avaient écrire un contrat en Praspel
avant cette expérimentation. Nous leur avons demandé de comparer le temps de
rédaction de la suite MT avec la suite AGT.

Pour la partie du panel qui n'était pas habituée à la programmation par contrat,
plusieurs heures (entre 4 et 10h) ont été nécessaires pour bien comprendre le
principe et savoir comment l'appliquer. Cette période comprend aussi l'analyse
des AGT~: savoir les lire, les comprendre et les positionner par rapport aux MT.
Durant cette période, écrire des MT était bien plus rapide que d'écrire des
contrats et produire des AGT. Pour l'autre partie du panel, même si elle
connaissait la programmation par contrat, elle ne l'avait jamais pratiquée. Les
problèmes rencontrés étaient les même~: savoir lire, comprendre et analyser les
AGT. Le panel pondère toutefois cet effort~: comparer aux autres outils qu'ils
manipulent quotidiennement, l'installation de Praspel et son extension pour
atoum est très rapide (autour d'une minute) et le langage est simple à utiliser.
Même si les notions sous-jacentes (les algorithmes et les méthodes du test
utilisées) ne sont pas connues ou comprises, les domaines réalistes sont
également une notion simple à comprendre et à utiliser.

Une fois le cap de l'apprentissage franchi, le panel a observé que pour peu de
code à tester (moins de 5 ou 6~méthodes), il était plus rapide d'écrire des MT.
En revanche, lorsqu'il faut tester plus de code (ce qui représente 95\% des
cas), l'utilisation des contrats s'avèrent très efficace. Il faut entre 2 et
4~fois moins de temps pour écrire les contrats et produire les AGT que d'écrire
les MT à la main (de 2h pour les MT à 30mn pour les AGT par exemple). Pour
mettre ces résultats en perspective avec la partie précédente, le temps gagné
est aussitôt réinvesti pour écrire des MT plus «~poussés~», comprendre plus
complets.

Le panel nous a en effet fait remarquer que malheureusemnt, les tests ne sont
pas encore une priorité dans les budgets alloués au développement logiciel. Par
conséquent, ils ont un temps limité pour tester leurs applications. Pendant le
temps imparti, ils n'étaient capable d'écrire que des tests dits «~basiques~» ou
alors ils ne testaient que les parties les plus critiques du code. Avec les
contrats, ils spécifient toutes les méthodes car ils sont écrits en même temps
que le code. La proximité entre le code et les contrats \via les annotations est
un avantage. Par conséquent, plus de code est testé sans nécessiter autant de
temps que si ça devait être fait manuellement. Le temps qui a été économisé est
ainsi réinvesti pour des tests plus complets, qui n'auraient pu être faits en
temps normal. Autrement dits, les tests «~usuels~» sont gérés par Praspel, ce
qui permet aux ingénieurs de test de se concentrer sur les parties plus
critiques.

Nous aurions aimé faire l'expérimentation 2~fois de suite pour comparer la
courbe d'apprentissage, mais nous n'en avons pas eu le temps. Nous rappelons que
le panel était constitué de bénévoles.

\subsection{Générations de données de tests}
%
%Les suites AGT ne sont pas toutes complètes. Il faut compléter avec des MT.
%Est-ce que les algos de génération de données sont utiles~? Sont-ils aussi
%efficaces~? Ont-ils réécrit des tests en utilisant la génération de données~?
%Quels résultats a-t-il apporté~?

Nous avons précisé dans les parties précédentes que les suites d'AGT ne sont pas
toujours complètes. En effet, Praspel n'est pas capable de tout spécifier et
certaines parties du code n'est pas toujours testable facilement. Heureusement,
la partie précédente nous a montré que le temps gagné grâce aux contrats est
réinvesti dans l'écriture de MT. Dans cette étape, les ingénieurs de test ont
utilisés nos générateurs de données. Rappelons qu'atoum ne possède aucun
générateur de données. Le seul outil qu'il propose consiste à se brancher sur un
dictionnaire de données écrit manuellement. Nous présentons trois constats.

\paragraph{Premier constat} Les domaines réalistes permettent de spécifier
simplement et facilement une grande majorité des données. Pouvoir utiliser les
domaines réalistes dans du code PHP directement permet de mixer et d'orienter la
génération facilement. Par exemple, écrire des boucles où chaque pas produit une
donnée légèrement différente de la précédente, ou encore utiliser des données
générées pour modifier d'autres données provenant de bases ou d'autres sources.

Par ailleurs, comme les données de tests ne sont plus déclarées manuellement,
cela simplifie grandement l'écriture des tests. Le panel a précisé que ce
n'était «~même pas comparable~»~: il n'y avait rien avant et c'était un travail
pénible, qui a maintenant disparu. Par ailleurs, cela simplifie aussi la lecture
des tests. En effet, le contenu du test se restreint à sa logique plutôt qu'à la
manipulation des données de tests. Le panel insiste sur le fait que les tests
sont par conséquent plus facilement maintenables. Cela implique que les
ingénieurs n'hésiteront pas à modifier les suites de tests car ce processus est
simplifié.

\paragraph{Deuxième constat} Sans générateur automatique de données, le panel
nous avoue que les données de tests utilisées étaient très peu nombreuses~:
entre 1 à 5 données par tests, 1.3 en moyenne. L'objectif était d'assurer une
certaine couverture du code avec le critère tous-les-arcs. Avec nos générateurs
de données, un grande nombre de données sont générables «~instantanément~» pour
reprendre leurs termes. En effet, le temps de génération est tellement rapide
qu'il en devient «~négligeable~». Le panel souligne qu'«~aucune différence n'est
perceptible~» entre l'exécution des suites de tests avec ou sans générateurs de
données\footnote{Nous noterons toutefois que les suites de tests ne comportaient
que quelques dizaines de tests.}. Dans certaines méthodologies de développement
comme le \inenglish{Test-Driven Development} ou les méthodes Agile, les tests
sont exécutés en permanence, plusieurs dizaines de fois par heure. La rapidité
d'exécution des tests est alors importante pour ne pas impacter la vitesse de
développement des équipes.

\paragraph{Troisième constat} Générer beaucoup de données de tests n'a aucun
sens si elles sont toutes inutiles. Le panel a à la majorité apprécié les
algorithmes de générations de chaînes de caractères. Pouvoir spécifier des
chaînes de caractères avec des expressions régulières ou des grammaires, et
ensuite générer toutes les données de manière exhaustive ou alors par couverture
de la grammaire, a été remarquablement apprécié. Plusieurs points sont à noter.

Tout d'abord, avoir une exhaustivité ou une couverture sur les données
manipulées est un avantage important~: cela augmente la confiance dans les
tests. La notion de couverture de données est aussi importante que la couverture
du code pour plus de la moitié de notre panel.

Ensuite, spécifier des chaînes de caractères avec des expressions régulières est
facilité par le fait que, très fréquemment, ces expressions régulières soient
déjà définies dans l'implémentation. Il suffit alors de les réutiliser.  Quand
nous avons demandé comment le panel générait des chaînes de caractères avant,
nous avons été surpris de voir qu'un seul ingénieur en générait en concaténant
des ensembles de sous-chaînes de caractères. Les autres écrivaient quelques
chaînes de caractères manuellement. Dans le cas des grammaires, un ingénieur a
fait remarquer qu'en plus d'apprendre le langage Praspel, il fallait apprendre
le langage PP pour décrire des grammaires. Les autres membres du panel n'ont pas
relevé cet inconvénient arguant qu'aux vues des services rendus, l'effort
demandé était très rapidement amorti.

Enfin, dans le cas des tableaux, le panel a apprécié le fait de pouvoir générer
à moindre coût des centaines de tableaux respectant certaines contraintes. Là
encore, très peu de tableaux étaient utilisés en tant que données de tests
manuellement avant. \\

Dans tous les cas, les membres du panel ont remarqué une diminution du nombre de
tests. En effet, comme un test manipule plus de données pertinentes, les autres
tests du même genre deviennent inutiles. Cette diminution a été difficile à
quantifier~: 5\% pour certains, 75\% pour d'autres, mais il y a toujours une
simplification de la suite de tests~: la quantité, la lecture, l'écriture et la
maintenance.

\subsection{Tests paramétrés}

%Certains auront probablement fait des tests paramétrés (ça se fait déjà un peu
%dans atoum, ça s'est fait dans UniTestor, je les encouragerai à le faire). Quels
%résultats~? C'est une approche «~hybride~», est-ce qu'elle comble certains
%manques de Praspel~? Ou même des MT~?

Le test paramétré est une approche hybride entre le test manuel et automatique.
Le principe est d'écrire un test dont les paramètres sont spécifiés par un
contrat. À partir d'un test paramétré, nous pouvons alors générer plusieurs
nouveaux tests. Ils permettent deux choses. La première est d'automatiser des
tests compliqués à générer automatiquement en décrivant un préambule de test
non-trivial ou en facilitant le travail de l'oracle. Par exemple, si le succès
ou l'échec d'un test est déterminé par des informations contenues dans un autre
programme ou dans une base de données, le test paramétré va synthétiser ces
informations de telle façon qu'elle soit spécifiable dans le contrat (par
conséquent avec Praspel). La seconde est d'utiliser des contrats dans un
contexte de test fonctionnel, c'est~à~dire que le contenu du test sera un
assemblage de plusieurs briques logiciels de plus haut niveau.

Durant cette expérimentation, seulement 1~ingénieur de test du panel a pu écrire
des tests paramétrés. L'aspect test fonctionnel n'a pas été abordé. Ce membre
n'a pas observé de différences particulières avec des MT dans lesquels il
utilisait les générateurs de données. La raison principal est que les contrats
qui annotaient les tests paramétrés se ramenaient à décrire le type des données
pour la précondition dans un seul et unique comportement et la postcondition
étaient très fréquemment \code{\aresult: true}. L'effort d'écriture du contrat
est donc équivalent à l'effort d'écriture d'un MT sauf que le processus de
générations des tests étaient plus compliqués. En effet, l'extension de Praspel
dans atoum est prévu à l'origine pour scanner des implémentations et non pas des
tests. Le problème était donc technique car il nécessitait d'instrumenter les
tests en plus des implémentations ce qui pouvait parfois poser des problèmes.

Cette partie de l'expérimentation s'est arrêtée sur ce constat~: les tests
paramétrés n'ont pas apporté de réels bénéfices dans ce contexte.

\subsection{Erreurs détectées}

Cette expérimentation a permi de révéler plusieurs erreurs dans tous les
programmes testés. Nous analysons les plus intéressantes d'entre elles.

\paragraph{Erreurs avec les dates} Les dates sont régulièrement des sources
d'erreurs dans plusieurs programmes. L'utilisation des domaines réalistes
\code{Date} et \code{Time\-stamp} permettent de générer et valider finement des
dates. Comme dans l'exemple de la partie~\ref{subsection:tools:extension}, une
erreur a été découverte avec des dates relatives comme entre «~le dernier
vendredi de février~» (spécifié avec \code{time\-stamp('last Friday of
February')}) et «~le lundi suivant~» (spécifié avec \code{time\-stamp('first
Monday of March')}). Certains cas avec des samedi et dimanche lors d'années
bissextiles étaient sources d'erreurs.

Une autre erreur a été détectée dans un programme qui formatait mal les dates.
Le mois de janvier était formaté en \code{1} au lieu de \code{01}. La donnée de
test manuelle portait sur le mois de décembre, mois durant lequel a été écrit le
test (l'ingénieur de test s'est inspiré du mois courant de l'époque pour écrire
la donnée de test). Lorsqu'il a spécifié sa donnée avec le domaine réaliste
\code{date('d')}, où \code{d} représente les mois sur deux chiffres, l'erreur
est apparue.

Une fois qu'un contrat ait été écrit ou qu'un générateur de données ait été
utilisé manuellement, 2~minutes en moyenne ont été nécessaire pour détecter ces
erreurs. Le code impacté n'était pas encore en production mais sur le point
d'être livré. L'ingénieur a estimé que si l'erreur était apparue en production,
son impact aurait été «~importante~».

\paragraph{Erreurs avec JSON} Un programme devait valider des données formatées
en JSON. Les données de tests manuelles ne couvraient pas tous les cas et
l'utilisation d'un générateur de chaînes de caractères exhaustif a permi de
détecter plusieurs erreurs. Elles ont été détectées en autant de temps qu'il
était nécessaire pour écrire le test, c'est~à~dire moins de 3~minutes. 

Ce programme n'était pas encore en production, il était en cours de
développement. L'impact des erreurs auraient pu être important puisqu'il
s'agissait de valider des données et qu'ils retournaient parfois des
faux-positifs et parfois des faux-négatifs (ce qui est moins grave dans ce
contexte). Il a fallu quelques minutes pour corriger les erreurs.

\paragraph{Erreurs arithmétiques} Un programme évaluait des formules
arithmétiques écrites par des utilisateurs pour caractériser l'évolution de
tarifs en temps-réel. Une erreur avec les divisions a été trouvée~: elles
étaient évaluées comme étant associatives gauches au lieu d'associatives
droites, seulement lorsque l'opérande droit avait une forme particulière. Cette
erreur a été détectée avec le générateur de chaînes de caractères exhaustif en
moins de 15~secondes une fois le test écrit.

Ce programme était en production, avec des centaines d'utilisateurs et plusieurs
centaines de milliers de clients (pratiquement un million). Des tests unitaires
et fonctionnells étaient même écrits et exécutés très régulièrement.
Heureusement, aucune formule de cette forme n'a été écrite par les utilisateurs.

\paragraph{Erreurs dans les tests} Dans les deux derniers points abordés, la
correction des erreurs a permis de détecter d'autres erreurs dans les tests. En
effet, les tests calculaient mal leurs verdicts. Praspel génèrait des AGT qui
échouaient alors que les MT équivalent étaient des succès. Après analyses des
AGT et des MT, il s'est avéré qu'il y avait des erreurs dans les MT.

\paragraph{Autres erreurs} D'autres erreurs ont été détectées notamment avec le
domaine réaliste \code{Regex} ou \code{Array} qui génèraient des données
non-prévues par l'implémentation. Ces erreurs ont principalement été détectées
en 2~minutes en moyenne.

\subsection{Retours des ingénieurs/expertise humaine}

Plutôt sous la forme d'une interview~? Est-ce que l'approche des contrats est
pertinente~? Est-ce que vous avez l'impression de gagner du temps~? De
l'argent~? De la qualité logiciel~? Qu'est-ce qu'il faudrait pour gagner en
maturité~? Les bugs qui ont été trouvés (s'il y en a), peut-on estimer leurs
coût~: en temps, en personne, en argent (par rapport à la criticité du bug)~?

Autre question, moins empirique~: est-ce que l'ingénieur aurait écrit ces tests
là, ou pas, des tests trop simples, des tests trop complexes, pertinents, ça
remplit nos objectifs qualités, de couverture etc.

Expressivité de Praspel~: est-ce que ce le langage est assez complet, est-ce
qu'ils peuvent tout exprimer avec ou pas~?

Est-ce qu'ils sont limités par les générateurs de données~? Est-ce qu'ils les
ont un peu hacker pour obtenir la donnée qu'ils voulaient~? Est-ce qu'ils
voulaient faire quelque chose qui n'était pas possible~? Est-ce qu'ils se
s'auraient poser la question sans Praspel~?

\subsubsection{Bénéfices des contrats}

Qu'est-ce que cela apporte à votre méthodologie~? Vous travaillez plus
lentement, plus rapidement~? Facile à prendre en main et à comprendre~?

\section{Autre cas d'études ponctuels}

Parler de ext/jsond (remplaçant de ext/json dans PHP)~? Test de non-régression
et de performance, basé sur notre article A-MOST'12.

Est-ce qu'ils ont trouvé des bugs dans Praspel~?

% En vrac~:
%     ext/jsond
%     Hoa\Compiler
%     Hoa\Math
%     Hoa\Ruler
%     Rezzza et son million d'utilisateur

\section{Synthèse}
\label{section:experimentation:other}
