\section{Ingénieurs bénévoles}
\label{section:experimentation:real}

L'expérimentation précédente a été réalisée auprès d'étudiants. Nous voulons
valider nos résultats auprès d'experts industriels sur des programmes du «~monde
réel~».

La partie~\ref{subsection:experimentation:presentation} présente les ingénieurs
bénévoles et les programmes sur lesquels ils ont joué l'expérimentation. La
partie~\ref{subsection:experimentation:modus_operandi} présente les protocoles
expérimentaux qu'ils ont appliqué. Les
parties~\ref{subsection:experimentation:coverage}
et~\ref{subsection:experimentation:time} présentent les résultats obtenus avec
la génération automatique de tests. La
partie~\ref{subsection:experimentation:data}, quant à elle, présente les
résultats obtenus avec uniquement la génération de données de test. Beaucoup
d'erreurs ont été détectées durant cette expérimentation. La
partie~\ref{subsection:experimentation:parameterized} en présente quelques-unes.
Enfin, cette expérimentation s'est achevée sur une discussion, synthétisée dans
la partie~\ref{subsection:experimentation:discuss}, concernant les bénéfices de
l'approche par les contrats pour le test, la pertinence de nos algorithmes de
génération de données, de Praspel et ses outils.

\subsection{Présentation générale}
\label{subsection:experimentation:presentation}

Nous avons lancé un appel sur différents réseaux sociaux et auprès de
différentes communautés pour rassembler des bénévoles pour cette
expérimentation. 7~ingénieurs de test de nationalités différentes ont répondu à
l'appel. Ces ingénieurs font du test depuis en moyenne 5~ans. Ils ont appliqué
l'expérimentation sur leurs propres programmes ou sur des programmes auxquels
ils participent (en tant que testeurs ou développeurs). Ces programmes
constituent nos cas concrets. Voici la description de ces programmes~:
%
\begin{itemize}

\item outil de \inenglish{debugging}~: analyse les performances d'exécutions de
certaines parties de programmes et génère des rapports dans deux formats
différents. Les parties abordées durant cette expérimentation étaient la mesure
des données et la génération des rapports.

\item outil de test pour le langage JSON~: une autre extension à atoum était en
développement pour proposer des nouvelles assertions sur le langage JSON. Les
parties abordées durant cette expérimentation étaient les assertions et les
tests de cette extension, c'est~à~dire s'assurer que l'outil de test testait
correctement le langage JSON.

\item outil d'échanges de messages~: les messages sont écrits en XML et
contiennent des données de toutes sortes, notamment des dates. C'est cette
partie qui a été majoritairement abordée durant cette expérimentation car
centrale dans cet outil.

\item générateur de données de test~: pour tester certaines parties techniques
d'un outil, un générateur de données de test spécifique a été créé. Afin de
s'assurer que ce générateur ne comporte pas d'erreurs et étant lui-même assez
volumineux, il a sa propre suite de tests. Les parties qui ont été abordées
durant cette expérimentation étaient le calcul de ces données, les tests
unitaires et les tests fonctionnels.

\item outils de comptabilités~: 2~outils développés sur mesure pour des calculs
de comptabilités. Les parties abordées étaient multiples.

\item outil d'évaluation de formules mathématiques~: des utilisateurs écrivent
eux-mêmes des formules arithmétiques décrivant l'évolution en temps réel des
tarifs de certains produits. Une bibliothèque de fonctions, de constantes et de
variables est mise à la disposition de ces utilisateurs. La partie abordée était
l'évaluation de ces formules analysées avec un compilateur et une grammaire
spécifiques.

\end{itemize}

Tous ces programmes font intervenir tous les types de données, à savoir des
booléens, des entiers, des réels, des chaînes de caractères, des tableaux et des
objets. La plupart des chaînes de caractères pouvaient être spécifiées par une
expression régulière, et quelques-unes par des grammaires.

Sur ces programmes variés, nous avons demandé à notre panel d'appliquer deux
protocoles expérimentaux.

\subsection{\inlatin{Modi operandi}}
\label{subsection:experimentation:modus_operandi}

Cette expérimentation s'est déroulée en deux temps. Tout d'abord, nous avons
demandé au panel d'appliquer le protocole expérimental suivant~:
%
\begin{enumerate}

\item sélectionner plusieurs méthodes de toutes sortes déjà testées
manuellement (avec atoum)~;

\item annoter ces méthodes avec des contrats Praspel~;

\item générer automatiquement une suite de tests satisfaisant le critère de
couverture de contrat \inenglish{All}-$G$, et l'exécuter~;

\item comparer la suite MT (\inenglish{Manual Tests}) avec la suite AGT
(\inenglish{Automatically Generated Tests}).

\end{enumerate}

Puis, nous leur avons demandé d'expérimenter les algorithmes de générations de
données seuls, sans Praspel. Nous rappelons que l'extension introduisant Praspel
dans atoum permet la définition et l'utilisation des domaines réalistes
facilement (voir la partie~\ref{subsection:tools:extension}). Il n'y avait pas
de protocole expérimental pré-défini mais nous leur avons demandé d'observer
l'impact de certains générateurs sur la couverture de code ainsi que l'impact
sur les suites de tests (par exemple une réduction du nombres de tests).

L'expérimentation a été ouverte pendant une première semaine avec un premier
panel puis pendant deux autres semaines avec un second panel plus important. Les
parties suivantes regroupent toutes les données et informations des deux panels.

\subsection{Couverture de code des suites de tests}
\label{subsection:experimentation:coverage}

Un moyen de comparer les suites de tests est d'utiliser la couverture de code
avec le critère tous-les-arcs, comme pour l'expérimentation précédente.  Les
résultats sont les suivants. Nous avons observé que les AGT couvrent autant le
code que les MT, à $\pm 5\%$ près.

Quand les AGT couvrent moins le code que les MT, le panel nous a fait remarquer
que c'était à cause de l'utilisation des bouchons (des \inenglish{mocks}). Un
bouchon remplace une partie d'un programme et nous prédéfinissons la manière
dont il va se comporter, c'est~à~dire que nous décrivons un comportement bien
spécifique et testons le SUT dans ces conditions.  Les bouchons permettent
d'améliorer la testabilité des programmes (en plus d'accélérer l'exécution des
suites de tests par effet de bord). Les bouchons ne concernent que les objets ou
les fonctions, et Praspel n'est pas capable de générer de tels bouchons. Par
exemple, si un contrat précise qu'une exception peut être levée quand une
connexion réseau est interrompue, Praspel ne sera pas capable de couper lui-même
la connexion. Il faut alors boucher les interfaces réseaux et simuler des
déconnexions.

Lorsque les AGT couvrent plus le code que les MT, c'est à cause d'une limitation
des outils. En effet, atoum ne permet pas de tester les méthodes avec une
visibilité autre que publique, alors que Praspel le permet. Le panel doit alors
tester les méthodes protégées et privées à travers les méthodes publiques. Il
est évident que certains chemins dans le code n'étaient pas accessibles
facilement.

Dans certains cas, malgré une couverture de code de 100\% pour les MT et les
AGT, des erreurs ont été trouvées. En effet, la couverture tous-les-arcs n'est
pas très fine et des erreurs peuvent apparaître derrière des conditions qui
n'auraient pas dû être activées. Comme le critère de couverture
\inenglish{All}-$G$ sur les contrats combine les domaines réalistes de toutes
les variables, certaines combinaisons peuvent activer ces conditions d'une
manière qui n'avait pas été testée manuellement.

Maintenant, comparons le temps de rédaction de ces suites de tests.

\subsection{Temps de rédaction des suites de tests}
\label{subsection:experimentation:time}

Tous les membres du panel sont habitués à écrire des tests. Ils en écrivent des
dizaines par jour. Aucun d'entre eux n'avait écrit un contrat en Praspel avant
cette expérimentation, mais certains connaissaient toutefois ce paradigme. Nous
leur avons demandé de comparer les temps de rédaction des suites MT et AGT.

Pour la partie du panel qui n'était pas habituée à la programmation par contrat,
plusieurs heures (entre 4 et 10h) ont été nécessaires pour bien comprendre le
principe et savoir comment l'appliquer. Cette période comprend aussi l'analyse
des AGT~: savoir les lire, les comprendre et les positionner par rapport aux MT.
Durant cette période, écrire des MT était bien plus rapide que d'écrire des
contrats et produire des AGT. Pour l'autre partie du panel, même si elle
connaissait la programmation par contrat, elle ne l'avait jamais pratiquée. Les
problèmes rencontrés étaient les mêmes~: savoir lire des contrats, comprendre et
analyser les AGT. Le panel pondère toutefois cet effort~: en comparaison avec
les autres outils qu'ils manipulent quotidiennement, l'installation de Praspel
et son extension pour atoum est très rapide (autour d'une minute) et le langage
est simple à utiliser. Même si les notions sous-jacentes (les algorithmes et
les techniques du test utilisés) ne sont pas connues ou comprises, les domaines
réalistes sont également une notion simple à comprendre et à utiliser.

Une fois le cap de l'apprentissage franchi, le panel a observé que pour peu de
code à tester (moins de 5 ou 6~méthodes), il était plus rapide d'écrire des MT.
En revanche, lorsqu'il faut tester plus de code (ce qui est pratiquement
toujours le cas), l'utilisation des contrats s'avère très efficace. Il faut
entre 2 à 4~fois moins de temps pour écrire les contrats et produire les AGT que
pour écrire les MT à la main (de 2h pour les MT à 30mn pour les AGT par
exemple). Pour mettre ces résultats en perspective avec la partie précédente, le
temps gagné est aussitôt réinvesti pour écrire des MT plus «~poussés~»,
comprendre plus complets.

Le panel nous a en effet fait remarquer que malheureusemnt, les tests ne sont
pas encore une priorité dans les budgets alloués au développement logiciel. Par
conséquent, les ingénieurs ont un temps limité pour tester leurs programmes.
Pendant le temps imparti, ils n'étaient capables d'écrire que des tests dits
«~basiques~» ou alors ils ne testaient que les parties les plus critiques du
code. Avec les contrats, ils spécifient toutes les méthodes car ils sont écrits
en même temps que le code. La proximité entre le code et les contrats \via les
annotations est un avantage. Par conséquent, plus de code est testé sans
nécessiter autant de temps que s'il devait être testé manuellement. Le temps qui
a été économisé est ainsi réinvesti pour des tests plus complets, qui n'auraient
pas pu être faits en temps normal. Autrement dit, les tests «~usuels~» sont
gérés par Praspel, ce qui permet aux ingénieurs de test de se concentrer sur les
parties plus critiques.

Nous aurions aimé faire l'expérimentation 2~fois de suite pour comparer la
courbe d'apprentissage, mais nous n'en avons pas eu le temps. Nous rappelons que
le panel était constitué de bénévoles.

\subsection{Génération de données de test}
\label{subsection:experimentation:data}

Nous avons précisé dans les parties précédentes que les suites d'AGT ne sont pas
toujours complètes. En effet, Praspel n'est pas capable de tout spécifier (nous
détaillerons plusieurs cas dans la suite de cette partie) et certaines parties
du code ne sont pas testables facilement. Heureusement, la partie précédente
nous a montré que le temps gagné grâce aux contrats est réinvesti dans
l'écriture de MT. Dans cette étape, les ingénieurs de test ont utilisés nos
générateurs de données. Rappelons qu'atoum ne possède aucun générateur de
données. Le seul outil qu'il propose consiste à se brancher sur un dictionnaire
de données écrit manuellement. Nous présentons trois constats.

\paragraph{Premier constat} Les domaines réalistes permettent de spécifier
simplement et facilement une grande majorité des données. Pouvoir utiliser les
domaines réalistes dans du code PHP directement permet de mixer et d'orienter la
génération facilement. Par exemple, écrire des boucles où chaque pas produit une
donnée légèrement différente de la précédente, ou encore utiliser des données
générées pour modifier d'autres données provenant de bases ou d'autres sources.

Par ailleurs, comme les données de test ne sont plus déclarées manuellement,
cela simplifie grandement l'écriture des tests. Le panel a précisé que ce
n'était «~même pas comparable~»~: il n'y avait rien avant et c'était un travail
pénible. De même, cela simplifie aussi la lecture des tests. En effet, le
contenu du test se restreint à sa logique plutôt qu'à la manipulation des
données de test. Le panel insiste sur le fait que les tests sont par conséquent
plus facilement maintenables. Cela implique que les ingénieurs n'hésiteront pas
à modifier les suites de tests car ce processus est simplifié.

\paragraph{Deuxième constat} Sans générateur automatique de données, le panel
nous avoue que les données de test utilisées étaient très peu nombreuses~:
entre 1 à 5 données par tests, 1.3 en moyenne. L'objectif était d'assurer une
certaine couverture du code avec le critère tous-les-arcs. Avec nos générateurs
de données, beaucoup de données sont générables «~instantanément~» pour
reprendre leurs termes. En effet, le temps de génération est suffisamment rapide
pour qu'il en devienne «~négligeable~». Le panel souligne qu'«~aucune différence
n'est perceptible~» entre l'exécution des suites de tests avec ou sans
générateurs de données. Dans certaines méthodologies de développement comme le
\inenglish{Test-Driven Development} ou les méthodes Agile, les tests sont
exécutés en permanence, plusieurs dizaines de fois par heure. La rapidité
d'exécution des tests est alors importante pour ne pas impacter la vitesse de
développement des équipes.

\paragraph{Troisième constat} Générer beaucoup de données de test n'a aucun
sens si elles sont toutes inutiles. Le panel a majoritairement apprécié les
algorithmes de génération de chaînes de caractères. Pouvoir spécifier des
chaînes de caractères avec des expressions régulières ou des grammaires, et
ensuite générer toutes les données de manière exhaustive ou alors par couverture
de la grammaire, a été remarquablement apprécié. Plusieurs points sont à noter.

Tout d'abord, avoir une exhaustivité ou une couverture sur les données
manipulées est un avantage important~: cela augmente la confiance dans les
tests. La notion de couverture de données est aussi importante que la couverture
du code pour plus de la moitié de notre panel.

Ensuite, spécifier des chaînes de caractères avec des expressions régulières est
facilité par le fait que, très fréquemment, ces expressions régulières sont déjà
définies dans l'implémentation. Il suffit alors de les réutiliser. Quand nous
avons demandé comment le panel générait des chaînes de caractères avant, nous
avons été surpris de voir qu'un seul ingénieur en générait en concaténant des
ensembles de sous-chaînes de caractères. Les autres écrivaient quelques chaînes
de caractères manuellement. Dans le cas des grammaires, un ingénieur a fait
remarquer qu'en plus d'apprendre le langage Praspel, il fallait apprendre le
langage PP pour décrire des grammaires. Les autres membres du panel n'ont pas
relevé cet inconvénient arguant que, vus les services rendus, l'effort demandé
était très rapidement amorti.

Enfin, dans le cas des tableaux, le panel a apprécié le fait de pouvoir générer
à moindre coût des centaines de tableaux respectant certaines contraintes. Là
encore, très peu de tableaux étaient utilisés avant en tant que données de test
manuels. \\

Dans tous les cas, les membres du panel ont remarqué une diminution du nombre de
tests. En effet, comme un test manipule plus de données pertinentes, les autres
tests du même genre deviennent inutiles. Cette diminution a été difficile à
quantifier~: 5\% pour certains, 75\% pour d'autres, mais il y a toujours une
simplification de la suite de tests~: la quantité, la lecture, l'écriture et la
maintenance.

\subsection{Tests paramétrés}
\label{subsection:experimentation:parameterized}

Le test paramétré est une approche hybride entre le test manuel et automatique.
Le principe est d'écrire un test dont les paramètres sont spécifiés par un
contrat. À partir d'un test paramétré, nous pouvons alors générer plusieurs
nouveaux tests. Un test annoté par un contrat et qui pré-calcule un verdict est
présenté dans la figure~\ref{figure:experimentation:parameterized}.
%
\begin{figure}

\begin{pre}
/** \\
 * \arequires i: 7..42 and \\
 *           j: /[a-i][1-5]+/; \\
 * \aensures  \aresult: true; \\
 */ \\
public function testSomething ( \$i, \$j ) \{ \\
 \\
    \$out = false; \\
 \\
    if(…) … \\
    // some asserts \\
    // compute \$out \\
 \\
    return \$out; \\
\}
\end{pre}

\caption{\label{figure:experimentation:parameterized} Un test paramétré.}

\end{figure}

Les tests paramétrés permettent deux choses. La première est d'automatiser des
tests compliqués à générer automatiquement en décrivant un préambule de test
non-trivial ou en facilitant le travail de l'oracle. Par exemple, si le succès
ou l'échec d'un test est déterminé par des informations contenues dans un autre
programme ou dans une base de données, le test paramétré va synthétiser ces
informations de telle façon que la postcondition soit spécifiable dans le
contrat (par conséquent avec Praspel). La seconde est d'utiliser des contrats
dans un contexte de test fonctionnel, c'est~à~dire que le contenu du test sera
un assemblage de plusieurs briques logicielles de plus haut niveau.

Durant cette expérimentation, seulement 1~ingénieur de test du panel a pu écrire
des tests paramétrés. L'aspect test fonctionnel n'a pas été abordé. Ce membre
n'a pas observé de différences particulières avec des MT dans lesquels il
utilisait les générateurs de données. La raison principale est que les contrats
qui annotaient les tests paramétrés se ramenaient à décrire le type des données
pour la précondition dans un seul et unique comportement et la postcondition
étaient très fréquemment \code{\aresult: true}. L'effort d'écriture du contrat
est donc équivalent à l'effort d'écriture d'un MT sauf que le processus de
génération des tests était plus compliqué. En effet, l'extension de Praspel dans
atoum est prévue à l'origine pour scanner des implémentations et non pas des
tests. Le problème était donc technique car il nécessitait d'instrumenter les
tests en plus des implémentations ce qui pouvait parfois poser des problèmes.

Cette partie de l'expérimentation s'est arrêtée sur ce constat~: les tests
paramétrés n'ont pas apporté de réels bénéfices dans ce contexte.

\subsection{Erreurs détectées}
\label{subsection:experimentation:errors}

Cette expérimentation a permis de révéler plusieurs erreurs dans tous les
programmes testés. Nous analysons les plus intéressantes d'entre elles.

\paragraph{Erreurs avec les dates} Les dates sont régulièrement des sources
d'erreurs dans plusieurs programmes. L'utilisation des domaines réalistes
\code{Date} et \code{Time\-stamp} permettent de générer et valider finement des
dates. Une erreur a été découverte avec des dates relatives comme entre «~le
dernier vendredi de février~» (spécifié avec \code{time\-stamp('last Friday of
February')}) et «~le lundi suivant~» (spécifié avec \code{time\-stamp('first
Monday of March')}). Certains cas avec des samedis et dimanches lors d'années
bissextiles étaient sources d'erreurs. L'erreur a été détectée par des gardes
dans le programme qui ont levé des exceptions.

Une autre erreur a été détectée dans un programme qui formatait mal les dates.
Le mois de janvier était formaté en \code{1} au lieu de \code{01}. La donnée de
test manuelle portait sur le mois de décembre, mois durant lequel a été écrit le
test (l'ingénieur de test s'est inspiré du mois courant de l'époque pour écrire
la donnée de test). Lorsqu'il a spécifié sa donnée avec le domaine réaliste
\code{date('d')}, où \code{d} représente les mois sur deux chiffres, l'erreur
est apparue. L'erreur a été détectée avec la clause \aensures des contrats.

Une fois qu'un contrat a été écrit ou qu'un générateur de données a été utilisé
manuellement, 2~minutes en moyenne sont nécessaires pour détecter ces erreurs.
Le code impacté n'était pas encore en production mais sur le point d'être livré.
L'ingénieur a estimé que si l'erreur était apparue en production, son impact
aurait été «~important~».

\paragraph{Erreurs avec JSON} Un programme devait valider des données formatées
en JSON. Les données de test manuelles ne couvraient pas tous les cas et
l'utilisation d'un générateur de chaînes de caractères exhaustif a permis de
détecter plusieurs erreurs. Elles ont été détectées en autant de temps qu'il
était nécessaire pour écrire le test, c'est~à~dire moins de 3~minutes. L'erreur
a été détectée par PHP à l'exécution lors d'opérations incomplètes.

Ce programme n'était pas encore en production, il était en cours de
développement. L'impact des erreurs aurait pu être important puisqu'il
s'agissait de valider des données et que le programme retournait parfois des
faux-positifs et parfois des faux-négatifs (ce qui est moins grave dans ce
contexte). Il a fallu quelques minutes pour corriger les erreurs.

\paragraph{Erreurs arithmétiques} Un programme évaluait des formules
arithmétiques écrites par des utilisateurs pour caractériser l'évolution de
tarifs en temps réel. Une erreur avec les divisions a été trouvée~: elles
étaient évaluées comme étant associatives gauches au lieu d'associatives
droites, seulement lorsque l'opérande droit avait une forme particulière. Cette
erreur a été détectée avec le générateur de chaînes de caractères exhaustif en
moins de 15~secondes une fois le test écrit.

Ce programme était en production, avec des centaines d'utilisateurs et plusieurs
centaines de milliers de clients (pratiquement un million). Des tests unitaires
et fonctionnels étaient même écrits et exécutés très régulièrement.
Heureusement, aucune formule de cette forme n'a été écrite par les utilisateurs.

\paragraph{Erreurs dans les tests} Dans les deux derniers points abordés, la
correction des erreurs a permis de détecter d'autres erreurs dans les tests. En
effet, les tests calculaient mal leurs verdicts ou étaient incomplets. Praspel
générait des AGT qui échouaient alors que les MT équivalent étaient des succès.
Après analyses des AGT et des MT, il s'est avéré qu'il y avait des erreurs dans
les MT. Les contrats avaient des postconditions simples (clause \aensures) mais
réparties dans plusieurs comportements (clause \abehavior). Les AGT reflétaient
toute la combinatoire possible alors que certains cas étaient oubliés dans les
MT. C'est la raison principale de ce genre d'erreurs dans les tests. Une autre
raison est que les AGT utilisent des générateurs de données plus complets que
les MT écrits sans nos générateurs de données. Un seul AGT exécuté plusieurs
fois couvrent alors plus de données qu'un MT. Lorsqu'un AGT détectait une erreur
sur les données, cette erreur n'était pas détectée dans les MT car la couverture
des données était nettement plus faible.

\paragraph{Autres erreurs} D'autres erreurs ont été détectées notamment avec les
domaines réalistes \code{Regex} et \code{Array} qui généraient des données non
prévues par l'implémentation. Ces erreurs ont principalement été détectées en
2~minutes en moyenne, grâce aux clauses \aensures des contrats.

\subsection{Expertise humaine~: discussion}
\label{subsection:experimentation:discuss}

Pour finir cette expérimentation, nous avons discuté en groupe avec les membres
du panel. Nous avons synthétisé ces discussions en plusieurs sujets.

\paragraph{Bénéfices des contrats} La remarque la plus fréquente était que les
con\-trats offrent une documentation plus complète. Ils sont écrits en même
temps que l'implémentation et sont utiles à court terme, alors qu'écrire une
documentation n'a pas de réel intérêt à court terme. Elle était rarement
écrite ou complète en plus d'être informelle. Cet aspect formel a été relevé
plusieurs fois~: le contrat se lit facilement et il n'est plus nécessaire de
regarder l'implémentation pour comprendre ce que fait la méthode.

Dans des équipes qui travaillent en \inenglish{Test-Driven Development},
c'est~à~dire que les tests sont écrits avant l'implémentation, il y a souvent
une notion de binômage. Les tests sont écrits par une personne et
l'implémentation par une autre. Cette approche fonctionne bien car les tests et
l'implémentation sont écrits dans des fichiers séparés. Avec les contrats, ils
sont écrits dans le même fichier. Il a donc été nécessaire de réorganiser le
mode de travail en échangeant le clavier. Nous avons fait remarquer au panel
qu'il était possible que les contrats soient écrits à part mais la proximité
entre le contrat et l'implémentation est désirée. Tout d'abord parce que le
contrat joue le rôle de documentation et que, d'autre part, il est plus facile
de maintenir un contrat lorsqu'il annote une méthode.

De plus, le panel a remarqué que l'écriture d'un contrat obligeait à réfléchir
davantage sur les données que manipule la méthode. Les tests obligent déjà à
réfléchir à l'architecture et aux API des programmes, mais les contrats vont
encore plus loin car ils nécessitent de formaliser ces réflexions. Le temps
dédié à l'écriture des contrats permettrait d'éviter des erreurs de conception
par la suite. Toutefois, nous n'avons pas réussi à observer ou quantifier ce
phénomène.

\paragraph{Bénéfices du \inenglish{Contract-based Testing}} Utiliser les
contrats pour générer des tests apporte principalement un gain de temps. Même si
les tests générés automatiquement sont considérés comme «~basiques~», ils
couvrent tout de même toute la spécification et sont pertinents. Le panel nous a
confirmé que la majorité des AGT auraient pu être écrits par eux-mêmes mais
qu'il leur aurait fallu beaucoup de temps. Le critère de couverture de contrat
\inenglish{All}-$G$ couvre toutes la structure du contrat (qui réflète plus ou
moins la structure de l'implémentation) et les combinaisons des données. Écrire
un contrat étant plus rapide que d'écrire les tests manuellement, le
\inenglish{Contract-based Testing} apporte un gain de temps et de confiance dans
les tests.

Le panel souligne encore que cette utilisation des contrats permet de «~déblayer
le terrain~» pour reprendre leurs termes. Ainsi, ils peuvent davantage se
concentrer sur des tests plus avancés tout en respectant le budget alloué à
cette phase du développement.

\paragraph{Génération de données de test} Utiliser des générateurs de données
permet de générer beaucoup de données pertinentes en très peu de temps. Le panel
a ressenti plus de confiance dans les tests avec les générateurs plutôt qu'avec
des données de test manuelles. Ce ressenti s'est confirmé par le nombre
d'erreurs détectées en peu de temps (en quelques minutes) dans des programmes
déjà testés.

L'utilisation de générateurs simplifie également l'écriture des tests, ainsi que
leur lecture et leur maintenance. Les générateurs permettent de réduire la
taille des suites de tests.

Ne pas connaître les algorithmes sous-jacents n'est pas réellement un problème.
Toutefois, utiliser ce type d'algorithmes pousse la réflexion plus loin. En
effet, certains des ingénieurs ont commencé à détourner certains algorithmes,
notamment sur les grammaires, pour répondre à des besoins très spécifiques. Ces
modifications ne sont pas à la portée de tout le monde et il a été demandé une
documentation et des informations détaillée sur ces sujets. Le panel a souligné
que sans Praspel, ils n'auraient même pas songé à ce genre de techniques. En
effet, comparé aux dictionnaires de données ou aux données manuelles qu'ils
utilisent quotidiennement, Praspel va plus loin et offre des services plus
efficaces.

\paragraph{À propos de Praspel} Praspel étant un projet de recherche, nous
n'avons pas eu le temps d'écrire une documentation utilisateur. Pour que le
panel puisse utiliser Praspel durant cette expérimentation, nous lui avons
fourni les articles existants~\acite{EnderlinDGO11, EnderlinDGB12,
EnderlinGB13}. Le panel a reconnu qu'une documentation utilisateur aurait été
plus appréciable mais que toutefois le langage était simple et intuitif. Il y a
peu de constructions, la syntaxe est simple et le fait qu'il soit possible de
composer des domaines réalistes est un réel avantage. Nous n'avons eu que très
peu de questions sur le langage.

Une remarque pertinente a été faite par un ingénieur. Praspel convient très bien
pour tester du code dit «~métier~», c'est~à~dire du code qui manipule des
données et qui en calcule d'autres. C'est exactement ce que fait le robot
UniTestor présenté dans la partie~\ref{section:experimentation:unitestor}. En
revanche, pour du code dit «~technique~», Praspel est plus limité. Un exemple de
code technique serait un générateur de bouchons (\inenglish{mock generator}). Un
tel générateur utilise un nom de classe en entrée et va générer du code PHP qui,
une fois exécuté, aura un comportement similaire à la classe bouchée {\em
modulo} certaines méthodes ou attributs avec des comportements différents.
Praspel pourrait valider que le code PHP produit est correct d'un point de vue
syntaxique, mais spécifier qu'il est correct d'un point de vue sémantique est
nettement plus difficile. Un autre exemple de code technique est un code créant
des \inenglish{threads}. Praspel est un incapable de spécifier que des
\inenglish{threads} ont été créés ou fermés. Actuellement, le seul moyen de
spécifier de telles postconditions se ferait par le biais d'un domaine réaliste
sur mesure ou alors en utilisant la construction \apred{\empty}. Si cet effort
est ponctuel, il est plus rapide d'écrire un test manuel. Cependant, un test
manuel pourrait être limité et l'utilisation d'un test paramétré (c'est~à~dire
annoté par un contrat, voir la
partie~\ref{subsection:experimentation:parameterized}) serait particulièrement
adaptée à ce genre de cas de figure.
